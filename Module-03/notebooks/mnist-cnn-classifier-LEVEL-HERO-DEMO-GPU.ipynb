{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12f43ca-d703-4737-adb6-2b0210ecc297",
   "metadata": {},
   "source": [
    "# MNIST Classifier - Hello World of ML!\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f1021-feb4-4fcf-ac18-f1770a739dd0",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is suffixed with __LEVEL_HERO_DEMO__ because it is deployable high performant ML classifier for the [MNIST](https://paperswithcode.com/dataset/mnist) dataset. This set is presented by your instructor during the lecture. Please confer to the lecture notes for further details.\n",
    "\n",
    "This notebook is a __demonstration notebook__ to: \n",
    "- test your DEV environment,\n",
    "- show you how developing a ML model can be very simple,\n",
    "- show you how a simple MLP can achieve quite-human performance in the simple task of recognizing hand-written digits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d918a0-7eeb-4ffb-961d-8fca8a000981",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a03d4f-fca3-4072-b511-5d70b3cccbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import idx2numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf ; tf.random.set_seed(42)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae7700a-1f3b-4b3b-981d-be0426f9c5b9",
   "metadata": {},
   "source": [
    "## Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd585e9b-03ea-41df-8dec-267a919d657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy\n",
    "\n",
    "np.set_printoptions(linewidth=200) # to enlarge the print() line\n",
    "np.random.seed(42) # the random seed init\n",
    "np.set_printoptions(precision=3) # for numpy floats: number of decimals\n",
    "\n",
    "# to suppress scientific notations like 1.500e+00 \n",
    "# np.set_printoptions(suppress=True)\n",
    "\n",
    "# TF\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # to disable TF debug logging messages "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0835150e-8aa4-44d5-af61-ff7704391d8c",
   "metadata": {},
   "source": [
    "## Globals & hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca2974f-7b59-4022-9285-349de938cf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============\n",
    "# DATA_TOPDIR\n",
    "# =============\n",
    "\n",
    "# Contain the (un)compressed idx files of MNIST\n",
    "\n",
    "# on assieoussou (my machine)\n",
    "DATA_TOPDIR = \"/home/ml/datasets/mnist\"\n",
    "\n",
    "# on your machine.... \n",
    "\n",
    "# =======\n",
    "# MNIST \n",
    "# =======\n",
    "\n",
    "# dataset files\n",
    "TRAIN_IMAGES_DATASET_FILE = os.path.join(DATA_TOPDIR, \"train-images-idx3-ubyte\")\n",
    "TRAIN_LABELS_DATASET_FILE = os.path.join(DATA_TOPDIR, \"train-labels-idx1-ubyte\")\n",
    "TEST_IMAGES_DATASET_FILE = os.path.join(DATA_TOPDIR, \"t10k-images-idx3-ubyte\")\n",
    "TEST_LABELS_DATASET_FILE = os.path.join(DATA_TOPDIR, \"t10k-labels-idx1-ubyte\")\n",
    "\n",
    "# The MNIST images format\n",
    "num_pixels = 28 * 28\n",
    "\n",
    "# the total number of digits\n",
    "num_classes = 10\n",
    "\n",
    "# ==========================\n",
    "# Training hyperparameters\n",
    "# ==========================\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "batch_size = 8 # <= on CPU\n",
    "# batch_size = 64 # <= on GPU\n",
    "\n",
    "# =======\n",
    "# Demo \n",
    "# =======\n",
    "\n",
    "# Demo dir: where demonstration images are placed\n",
    "DEMO_DIR = os.path.join(DATA_TOPDIR, \"demo\")\n",
    "os.makedirs(DEMO_DIR, exist_ok=True)\n",
    "\n",
    "# for demo images \n",
    "nb_demo = 10\n",
    "demo_prefix = \"demo_img_\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd193bc2-de05-4cff-94a6-a26fee328bd3",
   "metadata": {},
   "source": [
    "## Data Preparation (Part I)\n",
    "\n",
    ">__Note:__\n",
    ">\n",
    ">In this notebook, all the steps regarding the understanding of the data are skipped. Indeed, in practice Data Scientist spend __80%__ of their time here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd84508a-1edb-4154-af46-e123a46f5bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read each dataset into a conventional numpy 2D array\n",
    "train_x_ndarray = idx2numpy.convert_from_file(TRAIN_IMAGES_DATASET_FILE)\n",
    "train_y_ndarray = idx2numpy.convert_from_file(TRAIN_LABELS_DATASET_FILE)\n",
    "\n",
    "test_x_ndarray = idx2numpy.convert_from_file(TEST_IMAGES_DATASET_FILE)\n",
    "test_y_ndarray = idx2numpy.convert_from_file(TEST_LABELS_DATASET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2760f4-0690-4fa7-9769-c9bb70880185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check \n",
    "print(train_x_ndarray.shape, train_y_ndarray.shape) # => (60000, 28, 28) (60000,)\n",
    "print(f\"train_x_ndarray[0] = \\n{train_x_ndarray[0]}, \\ntrain_y_ndarray[0] = {train_y_ndarray[0]}\" )\n",
    "print(train_x_ndarray[0].shape, train_y_ndarray[0].shape) # => (28, 28) ()\n",
    "plt.imshow(train_x_ndarray[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f09af4-c9f1-4cad-a69a-bd8f4612f530",
   "metadata": {},
   "source": [
    "## Model construction & configuration\n",
    "\n",
    ">__Note:__\n",
    ">\n",
    ">In this notebook, all the steps regarding the (best) model selection and evaluation are skipped. We propose directly the best model architecture: a __Convolutional Neural Network (CNN)__ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851eea54-53e4-4cbc-bac4-884d3c798374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    # =================================\n",
    "    # Feature extractor\n",
    "    # =================================\n",
    "    model.add(Input(shape=(28,28,1))),\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5), activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2))),\n",
    "    model.add(Dropout(0.2))\n",
    "    # =================================\n",
    "    # Neck\n",
    "    # =================================\n",
    "    model.add(Flatten()),\n",
    "    # =================================\n",
    "    # Head \n",
    "    # =================================\n",
    "    model.add(Dense(units=64, activation=\"relu\")) # <= on CPU\n",
    "    # model.add(Dense(units=128, activation=\"relu\")) # <= on GPU\n",
    "    model.add(Dense(units=num_classes, activation=\"relu\"))\n",
    "    # =================================\n",
    "    # Compile model\n",
    "    # =================================\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Construct the model and show it\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67713258-9ee8-42ad-87ee-bdb1512fb6df",
   "metadata": {},
   "source": [
    "## Data Preparation (Part II)\n",
    "\n",
    ">__Note:__\n",
    ">\n",
    "> Now we know our taget ML model; thus we need to finalize the preparation of our data according this model (see the Input layer defined by `Input(shape=(28,28,1))`). Here, we have some image reshaping to do. Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bff6acc-1dcd-4acb-b8ea-5dbe8e3bb022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. no reshape is needed! Only a type cast is required\n",
    "X_train = train_x_ndarray.astype('float32')\n",
    "X_test = test_x_ndarray.astype('float32')\n",
    "\n",
    "# 2. normalization: VERY IMPORTANT!!!!\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# 3. one hot encoding of labels\n",
    "y_train = to_categorical(train_y_ndarray)\n",
    "y_test = to_categorical(test_y_ndarray)\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4fc4dc-c01f-4ff5-8677-c152c12f62d8",
   "metadata": {},
   "source": [
    "## Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4e84e-890c-41c3-a348-70c41724eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(\n",
    "    x=X_train, \n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_test, y_test), \n",
    "    epochs=epochs,   \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(\n",
    "    x=X_test, \n",
    "    y=y_test, \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\n[INFO] Model Val Accuracy: %.2f%%, Error: %.2f%%\" % (scores[1]*100, 100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc4711-84bc-4d7c-b7a2-e8bbc769e3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "019301cc-0bda-4729-ab11-34f1f3240495",
   "metadata": {},
   "source": [
    "## Construct the demonstration set\n",
    "\n",
    "Here, we isolate in the `demo/` subdirectory, some testing images. We'll use them later on - once our model is trained - to demonstrate how accurate it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eb1ad9-569a-427e-ad36-e3b5dbd59c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_demo_dir(nb, from_set, target_dir):\n",
    "    # randomly collect the indices\n",
    "    demo_rnd_indices = np.random.randint(1, high=len(from_set), size=nb)\n",
    "    for i in demo_rnd_indices:\n",
    "        plt.imsave(os.path.join(target_dir, demo_prefix + str(i) + \".jpg\"), from_set[i], cmap='gray')\n",
    "    print(f\"[INFO] {len(demo_rnd_indices)} images have been created in {target_dir}\")        \n",
    "\n",
    "# check if the demo dir exists and contains at least nb_demo files\n",
    "\n",
    "if os.path.exists(DEMO_DIR): \n",
    "    nb_files = len([name for name in os.listdir(DEMO_DIR) if os.path.isfile(os.path.join(DEMO_DIR, name))])\n",
    "    if nb_files < nb_demo:\n",
    "        # So you can safely manually add demo file \n",
    "        reconstruct_demo_dir(nb=nb_demo, from_set=test_x_ndarray, target_dir=DEMO_DIR)\n",
    "    else:\n",
    "        print(f\"[INFO] Nothing to do because {DEMO_DIR} contains already enough images!\")\n",
    "else: \n",
    "    reconstruct_demo_dir(nb=nb_demo, from_set=test_x_ndarray, target_dir=DEMO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a3be45-52e4-418e-beac-6ade65691659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel-venv-ai-dev-training-tf-py3.12.0",
   "language": "python",
   "name": "kernel-venv-ai-dev-training-tf-py3.12.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
