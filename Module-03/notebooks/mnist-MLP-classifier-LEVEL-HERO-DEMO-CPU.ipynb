{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12f43ca-d703-4737-adb6-2b0210ecc297",
   "metadata": {},
   "source": [
    "# MNIST Classifier Using an MLP - Hello World of ML!\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f1021-feb4-4fcf-ac18-f1770a739dd0",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is inffixed with __LEVEL HERO DEMO__ because it is deployable high performant ML classifier for the [MNIST](https://paperswithcode.com/dataset/mnist) dataset. This set is presented by your instructor during the lecture. Please confer to the lecture notes for further details.\n",
    "\n",
    "This notebook is a __demonstration notebook__ to: \n",
    "- test your DEV environment,\n",
    "- show you how developing a ML model can be very simple,\n",
    "- show you how a simple MLP can achieve quite-human performance in the simple task of recognizing hand-written digits.\n",
    "\n",
    ">__Note:__\n",
    ">\n",
    ">One can see the suffix __CPU__ to indicate that the NN architecture was lowered to allow a training on common CPU devices. Indeed, a __MLP__ is used here instead of a proper CNN architecture. Thus, the performances are downgraded compared to a __CNN__ model trained on a __GPU__ device. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d918a0-7eeb-4ffb-961d-8fca8a000981",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a03d4f-fca3-4072-b511-5d70b3cccbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 12:46:52.880574: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-06 12:46:52.884031: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-06 12:46:52.894774: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741265212.911992  413268 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741265212.916694  413268 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-06 12:46:52.933280: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import idx2numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf ; tf.random.set_seed(42)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae7700a-1f3b-4b3b-981d-be0426f9c5b9",
   "metadata": {},
   "source": [
    "## Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd585e9b-03ea-41df-8dec-267a919d657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy\n",
    "\n",
    "np.set_printoptions(linewidth=200) # to enlarge the print() line\n",
    "np.random.seed(42) # the random seed init\n",
    "np.set_printoptions(precision=3) # for numpy floats: number of decimals\n",
    "\n",
    "# to suppress scientific notations like 1.500e+00 \n",
    "# np.set_printoptions(suppress=True)\n",
    "\n",
    "# TF\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # to disable TF debug logging messages "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0835150e-8aa4-44d5-af61-ff7704391d8c",
   "metadata": {},
   "source": [
    "## Globals & hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ca2974f-7b59-4022-9285-349de938cf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============\n",
    "# DATA_TOPDIR\n",
    "# =============\n",
    "\n",
    "# Contain the (un)compressed idx files of MNIST\n",
    "\n",
    "# on assieoussou (my machine)\n",
    "DATA_TOPDIR = \"/home/ml/datasets/mnist\"\n",
    "\n",
    "# on your machine.... \n",
    "\n",
    "# =======\n",
    "# MNIST \n",
    "# =======\n",
    "\n",
    "# dataset files\n",
    "TRAIN_IMAGES_DATASET_FILE = os.path.join(DATA_TOPDIR, \"train-images-idx3-ubyte\")\n",
    "TRAIN_LABELS_DATASET_FILE = os.path.join(DATA_TOPDIR, \"train-labels-idx1-ubyte\")\n",
    "TEST_IMAGES_DATASET_FILE = os.path.join(DATA_TOPDIR, \"t10k-images-idx3-ubyte\")\n",
    "TEST_LABELS_DATASET_FILE = os.path.join(DATA_TOPDIR, \"t10k-labels-idx1-ubyte\")\n",
    "\n",
    "# The MNIST images format\n",
    "num_pixels = 28 * 28\n",
    "\n",
    "# the total number of digits\n",
    "num_classes = 10\n",
    "\n",
    "# ==========================\n",
    "# Training hyperparameters\n",
    "# ==========================\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 2 # <= on CPU\n",
    "\n",
    "# =======\n",
    "# Demo \n",
    "# =======\n",
    "\n",
    "# Demo dir: where demonstration images are placed\n",
    "DEMO_DIR = os.path.join(DATA_TOPDIR, \"demo\")\n",
    "os.makedirs(DEMO_DIR, exist_ok=True)\n",
    "\n",
    "# for demo images \n",
    "nb_demo = 10\n",
    "demo_prefix = \"demo_img_\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd193bc2-de05-4cff-94a6-a26fee328bd3",
   "metadata": {},
   "source": [
    "## Data Preparation (Part I)\n",
    "\n",
    ">__Note:__\n",
    ">\n",
    ">In this notebook, all the steps regarding the understanding of the data are skipped. Indeed, in practice Data Scientist spend __80%__ of their time here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd84508a-1edb-4154-af46-e123a46f5bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28) (50000,)\n",
      "(10000, 28, 28) (10000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# 1. Read each dataset into a conventional numpy 2D array\n",
    "train_x_ndarray = idx2numpy.convert_from_file(TRAIN_IMAGES_DATASET_FILE)\n",
    "train_y_ndarray = idx2numpy.convert_from_file(TRAIN_LABELS_DATASET_FILE)\n",
    "\n",
    "test_x_ndarray = idx2numpy.convert_from_file(TEST_IMAGES_DATASET_FILE)\n",
    "test_y_ndarray = idx2numpy.convert_from_file(TEST_LABELS_DATASET_FILE)\n",
    "\n",
    "# let us create a validation set from the training one: \n",
    "\n",
    "valid_size = 10000\n",
    "valid_x_ndarray = train_x_ndarray[:valid_size]\n",
    "valid_y_ndarray = train_y_ndarray[:valid_size]\n",
    "\n",
    "# remove from train\n",
    "train_x_ndarray = train_x_ndarray[valid_size:]\n",
    "train_y_ndarray = train_y_ndarray[valid_size:]\n",
    "\n",
    "print(train_x_ndarray.shape, train_y_ndarray.shape)\n",
    "print(valid_x_ndarray.shape, valid_y_ndarray.shape)\n",
    "print(test_x_ndarray.shape, test_y_ndarray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d2760f4-0690-4fa7-9769-c9bb70880185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_ndarray[0] = \n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  29 150 195 254 255 254 176 193 150  96   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  48 166 224 253 253 234 196 253 253 253 253 233   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0  93 244 249 253 187  46  10   8   4  10 194 253 253 233   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 107 253 253 230  48   0   0   0   0   0 192 253 253 156   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   3  20  20  15   0   0   0   0   0  43 224 253 245  74   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 249 253 245 126   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  14 101 223 253 248 124   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  11 166 239 253 253 253 187  30   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  16 248 250 253 253 253 253 232 213 111   2   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  43  98  98 208 253 253 253 253 187  22   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   9  51 119 253 253 253  76   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1 183 253 253 139   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 182 253 253 104   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  85 249 253 253  36   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  60 214 253 253 173  11   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  98 247 253 253 226   9   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  42 150 252 253 253 233  53   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  42 115  42  60 115 159 240 253 253 250 175  25   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 187 253 253 253 253 253 253 253 197  86   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 103 253 253 253 253 253 232  67   1   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]], \n",
      "train_y_ndarray[0] = 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# quick check \n",
    "plt.imshow(train_x_ndarray[0], cmap='gray')\n",
    "print(f\"train_x_ndarray[0] = \\n{train_x_ndarray[0]}, \\ntrain_y_ndarray[0] = {train_y_ndarray[0]}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f09af4-c9f1-4cad-a69a-bd8f4612f530",
   "metadata": {},
   "source": [
    "## Model construction & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "851eea54-53e4-4cbc-bac4-884d3c798374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kla/Workspace/training/become-ai-dev-part-1/Module-03/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-03-06 12:46:56.071786: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m50,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,890</span> (198.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,890\u001b[0m (198.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,890</span> (198.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,890\u001b[0m (198.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    # =================================\n",
    "    # Feature extractor\n",
    "    # =================================\n",
    "    model.add(Dense(\n",
    "        units=64, \n",
    "        input_dim=num_pixels, # <=> input_shape=(num_pixels,),\n",
    "        activation=\"relu\")\n",
    "    )   \n",
    "    # =================================\n",
    "    # Head/Output \n",
    "    # =================================\n",
    "    model.add(Dense(units=num_classes, activation=\"relu\"))\n",
    "    # =================================\n",
    "    # Compile model\n",
    "    # =================================\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Construct the model and show it\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67713258-9ee8-42ad-87ee-bdb1512fb6df",
   "metadata": {},
   "source": [
    "## Data Preparation (Part II)\n",
    "\n",
    ">__Note:__\n",
    ">\n",
    "> Now we know our taget ML model; thus we need to finalize the preparation of our data according this model (see the Input layer defined by `input_dim=num_pixels=28*28`). We don't have to do a lot of work since the original images format is quite similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bff6acc-1dcd-4acb-b8ea-5dbe8e3bb022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000, 10)\n",
      "(10000, 784) (10000, 10)\n",
      "(10000, 784) (50000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 1. reshape\n",
    "X_train = train_x_ndarray.reshape(train_x_ndarray.shape[0], num_pixels).astype('float32')\n",
    "X_valid = valid_x_ndarray.reshape(valid_x_ndarray.shape[0], num_pixels).astype('float32')\n",
    "X_test = test_x_ndarray.reshape(test_x_ndarray.shape[0], num_pixels).astype('float32')\n",
    "\n",
    "# 2. normalization\n",
    "X_train = X_train / 255\n",
    "X_valid = X_valid / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# 3. one hot encoding\n",
    "y_train = to_categorical(train_y_ndarray)\n",
    "y_valid = to_categorical(valid_y_ndarray)\n",
    "y_test = to_categorical(test_y_ndarray)\n",
    "\n",
    "assert y_train.shape[1] == num_classes, f\"[FATAL] The number of classes should be equal to {num_classes}. Abort!\"\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_valid.shape, y_valid.shape)\n",
    "print(X_test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4fc4dc-c01f-4ff5-8677-c152c12f62d8",
   "metadata": {},
   "source": [
    "## Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dd4e84e-890c-41c3-a348-70c41724eda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25000/25000 - 34s - 1ms/step - accuracy: 0.0984 - loss: nan\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# validation_data=(X_valid, y_valid), \u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Final evaluation of the model\u001b[39;00m\n\u001b[32m     12\u001b[39m scores = model.evaluate(\n\u001b[32m     13\u001b[39m     x=X_test, \n\u001b[32m     14\u001b[39m     y=y_test, \n\u001b[32m     15\u001b[39m     verbose=\u001b[32m0\u001b[39m\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/training/become-ai-dev-part-1/Module-03/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/training/become-ai-dev-part-1/Module-03/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/training/become-ai-dev-part-1/Module-03/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/training/become-ai-dev-part-1/Module-03/.venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/training/become-ai-dev-part-1/Module-03/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/training/become-ai-dev-part-1/Module-03/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/training/become-ai-dev-part-1/Module-03/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/training/become-ai-dev-part-1/Module-03/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/training/become-ai-dev-part-1/Module-03/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/training/become-ai-dev-part-1/Module-03/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/training/become-ai-dev-part-1/Module-03/.venv/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1683\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1681\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1682\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1683\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1684\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1688\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1691\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1692\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1693\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1697\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1698\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/training/become-ai-dev-part-1/Module-03/.venv/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(\n",
    "    x=X_train, \n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_valid, y_valid), \n",
    "    epochs=epochs,   \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(\n",
    "    x=X_test, \n",
    "    y=y_test, \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\n[INFO] Model Val Accuracy: %.2f%%, Error: %.2f%%\" % (scores[1]*100, 100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019301cc-0bda-4729-ab11-34f1f3240495",
   "metadata": {},
   "source": [
    "## Construct the demonstration set\n",
    "\n",
    "Here, we isolate in the `demo/` subdirectory, some testing images. We'll use them later on - once our model is trained - to demonstrate how accurate it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eb1ad9-569a-427e-ad36-e3b5dbd59c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_demo_dir(nb, from_set, target_dir):\n",
    "    # randomly collect the indices\n",
    "    demo_rnd_indices = np.random.randint(1, high=len(from_set), size=nb)\n",
    "    for i in demo_rnd_indices:\n",
    "        plt.imsave(os.path.join(target_dir, demo_prefix + str(i) + \".jpg\"), from_set[i], cmap='gray')\n",
    "    print(f\"[INFO] {len(demo_rnd_indices)} images have been created in {target_dir}\")        \n",
    "\n",
    "# check if the demo dir exists and contains at least nb_demo files\n",
    "\n",
    "if os.path.exists(DEMO_DIR): \n",
    "    nb_files = len([name for name in os.listdir(DEMO_DIR) if os.path.isfile(os.path.join(DEMO_DIR, name))])\n",
    "    if nb_files < nb_demo:\n",
    "        # So you can safely manually add demo file \n",
    "        reconstruct_demo_dir(nb=nb_demo, from_set=test_x_ndarray, target_dir=DEMO_DIR)\n",
    "    else:\n",
    "        print(f\"[INFO] Nothing to do because {DEMO_DIR} contains already enough images!\")\n",
    "else: \n",
    "    reconstruct_demo_dir(nb=nb_demo, from_set=test_x_ndarray, target_dir=DEMO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a3be45-52e4-418e-beac-6ade65691659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel-venv-ai-dev-training-tf-py3.12.0",
   "language": "python",
   "name": "kernel-venv-ai-dev-training-tf-py3.12.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
